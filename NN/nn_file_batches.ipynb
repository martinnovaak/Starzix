{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ea18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataIntoFiles():\n",
    "    f = open(\"sf_d9.plain\", \"r\")\n",
    "    linesPerFile = 100002\n",
    "    myDir = \"sf_d9_data/\"\n",
    "\n",
    "    linesProcessed = 0\n",
    "    fileNum = 0\n",
    "    currentFileName = myDir + \"sf_d9_0.plain\"\n",
    "    currentFile = open(currentFileName, \"w\")\n",
    "    for line in f:\n",
    "        currentFile.write(line)\n",
    "        linesProcessed += 1\n",
    "        if linesProcessed == linesPerFile:\n",
    "            print(\"Finished file \" + currentFileName)\n",
    "            linesProcessed = 0\n",
    "            fileNum += 1\n",
    "            currentFileName = myDir + \"sf_d9_\" + str(fileNum) + \".plain\"\n",
    "            currentFile = open(currentFileName, \"w\")\n",
    "            \n",
    "#splitDataIntoFiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd06043c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fileNames = []\n",
    "for fileName in os.listdir(\"sf_d9_data\"):\n",
    "    fileNames.append(\"sf_d9_data/\" + fileName)\n",
    "numFiles = len(fileNames)\n",
    "print(\"Files:\", numFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab3c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def getFensAndScores(fileName):\n",
    "    f = open(fileName, \"r\")\n",
    "    lines = f.readlines()\n",
    "    leng = len(lines)\n",
    "    i = 0\n",
    "    fens = []\n",
    "    scores = []\n",
    "    while i < leng and i+2 < leng:\n",
    "        lines[i] = lines[i].strip() # lines[i] is \"fen <fen>\"\n",
    "        lines[i+2] = lines[i+2].strip() # lines[i+2] is \"score -529\"\n",
    "\n",
    "        fen = lines[i][4:] # lines[i] is \"fen <fen>\"\n",
    "\n",
    "        score = int(lines[i+2][6:]) # lines[i+2] is \"score -529\"\n",
    "        if score > 1000:\n",
    "            score = 1000\n",
    "        if score < -1000:\n",
    "            score = -1000\n",
    "        if \" b \" in fen:\n",
    "            score = -score # scores in the file are stm, mirror black ones to get white perspective\n",
    "\n",
    "        fens.append(fen)\n",
    "        scores.append(score)\n",
    "        i += random.randint(10, 20) * 6 # jump to next random fen (\"fen <fen>\" line is every 6 lines)\n",
    "    return fens, scores\n",
    "\n",
    "def countPositions():\n",
    "    total = 0\n",
    "    for fileName in fileNames:\n",
    "        fens, scores = getFensAndScores(fileName)\n",
    "        total += len(fens)\n",
    "    print(\"Positions:\", total)\n",
    "    \n",
    "#countPositions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4446db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "\n",
    "def fen_to_12x8x8(myFen,  flip=True, debug=False):\n",
    "    board = chess.Board(myFen)\n",
    "        \n",
    "    pieces = [chess.PAWN, chess.KNIGHT, chess.BISHOP, chess.ROOK, chess.QUEEN, chess.KING]\n",
    "    array = np.empty((12,8,8), dtype=int)\n",
    "    i = 0 \n",
    "    for color in [chess.WHITE, chess.BLACK]:\n",
    "        for piece in pieces:\n",
    "            bitboard = np.asarray(board.pieces(piece, color).tolist()).astype(int)\n",
    "        \n",
    "            bitboard = bitboard.reshape(8,8)\n",
    "            array[i] = bitboard\n",
    "            i += 1\n",
    "            \n",
    "    return array\n",
    "\n",
    "# rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\n",
    "fen_to_12x8x8(\"7k/7p/6p1/4pp2/4P3/8/3Q1PPP/7K w - - 0 1\", debug=True)\n",
    "fen_to_12x8x8(\"7k/7p/6p1/4pp2/4P3/8/3Q1PPP/7K b - - 0 1\", debug=True)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class TrainDataGenerator(Sequence):\n",
    "    def __init__(self):\n",
    "        self.offset = int(numFiles*0.2)\n",
    "        pass\n",
    "    \n",
    "    def __len__(self): # num of batches in an epoch\n",
    "        return numFiles - self.offset\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fens, scores = getFensAndScores(fileNames[self.offset + index])\n",
    "        bbs = [fen_to_12x8x8(fen) for fen in fens]\n",
    "        return np.array(bbs), np.array(scores)\n",
    "    \n",
    "class ValDataGenerator(Sequence):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self): # num of batches in an epoch\n",
    "        return int(numFiles*0.2)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fens, scores = getFensAndScores(fileNames[index])\n",
    "        bbs = [fen_to_12x8x8(fen) for fen in fens]\n",
    "        return np.array(bbs), np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6cc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# mean(abs(tanh(output/400) - tanh(cp/400)))\n",
    "def custom_loss(y_true, y_pred):\n",
    "    from tensorflow import reduce_mean, tanh, cast, float32, abs, reduce_sum\n",
    "    y_true = cast(y_true, dtype=float32)  # Convert y_true to float32\n",
    "    y_pred = cast(y_pred, dtype=float32)  # Convert y_pred to float32\n",
    "    tanh_output = tanh(y_pred / 400)\n",
    "    tanh_cp = tanh(y_true / 400)\n",
    "    abs_diff = abs(tanh_output - tanh_cp)\n",
    "    loss = reduce_mean(abs_diff)\n",
    "    return loss\n",
    "\n",
    "def train(fileName, lr):\n",
    "    from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
    "    from tensorflow.keras.models import Model, Sequential\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(name=\"flatten\", input_shape=(12, 8, 8)))\n",
    "    model.add(Dense(name=\"dense32\", units=32, activation='relu'))\n",
    "    model.add(Dense(name=\"dense1\", units=1, activation='linear'))\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(loss=custom_loss, optimizer=Adam(learning_rate=lr)) # default lr 0.001\n",
    "    train_generator = TrainDataGenerator()\n",
    "    validation_generator = ValDataGenerator()\n",
    "    history = model.fit(train_generator, epochs=30, validation_data=validation_generator,\n",
    "                       callbacks=[EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)])\n",
    "    \n",
    "    model.save(fileName)\n",
    "    with open(fileName[:-3] + \"_history\", 'wb') as hist:\n",
    "        pickle.dump(history.history, hist) \n",
    "    \n",
    "#train(\"model_0.01.h5\", 0.01)\n",
    "train(\"model_0.001.h5\", 0.001)\n",
    "#train(\"model_0.0001.h5\", 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "    \n",
    "model = load_model(\"model.h5\", custom_objects={'custom_loss': custom_loss})\n",
    "with open(\"history\", \"rb\") as hist_file:\n",
    "    history = pickle.load(hist_file)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074529c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    import matplotlib.pyplot as plt\n",
    "    global history\n",
    "    if not isinstance(history, dict):\n",
    "        history = history.history\n",
    "    # Plot training and validation loss over epochs\n",
    "    plt.plot(history['loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f85082",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "model.evaluate(X_test, y_test, batch_size=64)\n",
    "\n",
    "totalLoss = 0\n",
    "i = 0\n",
    "for _769 in X_test[:1000]:\n",
    "    _769 = _769.reshape(1, 769,1)\n",
    "    pred = model.predict(_769, verbose=0)[0][0]\n",
    "    totalLoss += abs(pred - y_test[i])\n",
    "    i += 1\n",
    "print(\"cp loss:\", totalLoss / 1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f043f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
